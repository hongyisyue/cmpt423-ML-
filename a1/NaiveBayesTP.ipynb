{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier: Theory\n",
    "\n",
    "Based on two main ideas:\n",
    "* Bayes' Rule\n",
    "* Strong assumption of conditional independence.  \n",
    "\n",
    "We assume \n",
    "* The class variable $Y$ is a discrete random variable, which we want to predict.\n",
    "* $Y$ has at least 2 values, $\\{0,1,\\ldots, u-1 \\}, u > 1$.\n",
    "* The basis for making a prediction is a set of random variables called features $X$.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classifier function, from a probabilistic perspective:\n",
    "* Notated by $y(x)$\n",
    "* Reads intuitively: The predicted value of $Y$ for the given observation of $X=x$\n",
    "\n",
    "$$ y(x) = \\arg \\max_{k} \\left\\{P(Y=y_k | X=x ) \\right\\}$$\n",
    "\n",
    "**Note** This is a general form for classification from a probabilistic perspective, not specific to Naive Bayes.  The observation, $X=x$ could be anything: a single variable-value pair, or a conjunction of variable-value pairs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the Naive Bayes classifier:\n",
    "\n",
    "Applying Bayes Rule:\n",
    "    $$ P(Y=y_k | X=x ) = \\frac{P(X=x|Y=y_k)P(Y=y_k)}{P(X=x)}  $$\n",
    "\n",
    "\n",
    "We will assume that $X=x$ is an abbreviation for a conjunction of variable-value pairs.  In other words, we may have several observations $(X_a=x_a \\wedge X_b=x_b \\wedge  X_c=x_c \\wedge \\ldots)$, for as many variables as the problem defines.  We'll use $X_i = x_{ij}$ to represent any single feature and its observed value.\n",
    "\n",
    "Naive Bayes Conditional Independence Assumption: Every feature $X_a$ is conditionally independent of every other feature $X_b$, given $Y$. Mathematically:\n",
    "\n",
    " $$P(X=x|Y=y_k) = \\prod_{i=1}^{p} P(X_i=x_{ij}|Y=y_k) $$\n",
    "\n",
    "So the Naive Bayes Classifier is defined as follows:\n",
    "\n",
    "$$ y(x) = \\arg \\max_{k} \\left\\{ \\frac{P(Y=y_k) \\prod_{i=1}^{p} P(X_i=x_{ij}|Y=y_k)}{P(X=x)} \\right\\}$$\n",
    "\n",
    "Since $P(X=x)$ is constant:\n",
    "\n",
    "$$ y(x) = \\arg \\max_{k} \\left\\{ P(Y=y_k) \\prod_{i=1}^{p} P(X_i=x_{ij}|Y=y_k) \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Naive Bayes Model for Classification\n",
    "\n",
    "1. Given observation $X=x$\n",
    "2. Compute $$P(Y=y_k) \\prod_{i=1}^{p} P(X_i=x_{ij}|Y=y_k) $$ for every $y_k$.  \n",
    "2. Choose the value $y_k$ that resulted in the highest calculated value.\n",
    "\n",
    "Classification using Naive Bayes requires $O(up)$ multiplications, where $p$ is the number of features $\\{X_i\\}$, and $u$ is the number of discrete values for $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generality of the Naive Bayes Model\n",
    "The Naive Bayes Model makes one very strong assumption: that the features $X_i$ are conditionally independent given $Y$.  We will address this assumption later.\n",
    "\n",
    "The Naive Bayes Model only applies to classification tasks, and so by definition, $Y$ is a discrete random variable.  When $Y$ is a continuous random variable, it's not a classification task, it's regression, and a different topic.  \n",
    "\n",
    "The Naive Bayes Model above makes no assumptions about what kinds of relationships could exist between $Y$ and any of the $X_i$.  Typically, software that implements Naive Bayes assumes a common form for $P(X_i=x_{ij}|Y=y_k)$ for all of the $X_i$, e.g., they are all Conditional Gaussian distributions, but with different means and variances, or they are all binary random variables.  This is not a necessary assumption.  Because of the strong conditional independence assumption, we can consider each distribution $P(X_i=x_{ij}|Y=y_k)$ independently.\n",
    "\n",
    "In the following sections, we'll look at how to handle $P(X_i=x_{ij}|Y=y_k)$ when $X_i$ is a discrete variable, and when it's a continuous variable.  Inferring $P(X_i=x_{ij}|Y=y_k)$ from data $\\mathbf{X}$ is a density estimation task.\n",
    "\n",
    "We assume a dataset notated by $\\mathbf{X}$ \n",
    "* Sorry, the observation $X$ and the dataset $\\mathbf{X}$ are different!\n",
    "* $\\mathbf{X}$ consists of samples $x, y$ that we can use to build the classifier. \n",
    "* Here $x$ abbreviates a conjunction of variable-value pairs, similar to the above.  \n",
    "* These samples are assumed to be collected independently from the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(Y)$ from Discrete data when $Y$ is a Binary Random Variable\n",
    "\n",
    "We use the data to estimate the model parameters $\\theta_k$:\n",
    "\n",
    "$$ \\theta_k   =  P(Y=y_k) $$\n",
    "\n",
    "Here, $k$ is 0 or 1 (binary).\n",
    "\n",
    "#### Using Maximum Likelihood Estimates \n",
    "We can estimate $P(Y=y_k)$ by counting how often $Y=y_k$ appears in the data $\\mathbf{X}$:\n",
    "\n",
    "$$ \\theta_k = \\cdots =  \\frac{\\#\\mathbf{X}(Y=y_k)}{N} $$\n",
    "\n",
    "where $\\#\\mathbf{X}(Y=y_k)$ means the count of $Y=y_k$ in the data $\\mathbf{X}$.  \n",
    "\n",
    "The ellipsis $= \\cdots =$ in the formula above is used in place of the derivation that we saw in a previous lecture.  You know, likelihood, log likelihood, derivative set to zero, solving.  We've seen it earlier, so it was omitted here.\n",
    "\n",
    "\n",
    "#### Using the Bayesian Approach\n",
    "$$ \\theta_k   = \\cdots = \\frac{\\#\\mathbf{X}(Y=y_k) + a}{N + a + b } $$\n",
    "\n",
    "where: \n",
    "* $\\#\\mathbf{X}(Y=y_k)$ means the count of $Y=y_k$ in the data $\\mathbf{X}$.\n",
    "* The values $a$ and $b$ are the result of using a $\\mathrm{Beta}(a,b)$ prior for $\\theta_k$.  See below.\n",
    "\n",
    "\n",
    "\n",
    "This formula comes from the approach of assuming a Beta prior distribution over each of the parameters $\\theta_k$, and then calculating $E \\left[  \\theta_k | \\mathbf{X} \\right]$.  That derivation can be found in a previous lecture, and it was represented by $= \\cdots =$ above.  \n",
    "\n",
    "The Bayesian approach is a way that we can include prior knowledge we might have about any (or all) of the $\\theta_k$ parameters before we fit the data.  Using values $a=b=1$ is very common, and reflects the assumption that $\\theta_k$ could be any value in the range $[0,1]$.  Other names for this include:\n",
    "* Laplacian smoothing\n",
    "* add-one smoothing\n",
    "\n",
    "and their effect is to reduce the worst consequences of over-fitting when the fitting process involves counting.  It's probably unwise to give probability zero to an event that has a zero count in the data $\\mathbf{X}$.  The values $a$ and $b$ are sometimes called *pseudo-counts*, reflecting how they are added to real counted data in he formulae.  Each  random variable $X_i$ and $Y$  gets a pair of pseudo-counts, one for each possible outcome.  \n",
    "\n",
    "When there is a lot of data, the effect of these $a$ and $b$ values will be small, and will converge with the maximum likelihood estimates for $\\theta_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(X_i|Y)$ from Discrete data when $X_i$ is a Binary Random Variable\n",
    "\n",
    "The discrete probability distribution $P(X_i|Y)$ has a finite number of point probabilities.  We will represent them using $\\theta_{ijk}$ as follows:\n",
    "$$ \\theta_{ijk} = P(X_i = x_{ij}|Y=y_k)$$\n",
    "\n",
    "From here, we can take the same approach as we saw for $P(Y)$ above.  Each $\\theta$ is estimated by looking at the data.  The only difference is that we have to keep track of different ways that $X_i$ behaves for each value $Y=y_k$.  \n",
    "\n",
    "#### Using Maximum Likelihood Estimates \n",
    "We can estimate $P(X_i = x_{ij}| Y=y_k)$ by counting how often the event $(X_i = x_{ij} \\wedge Y=y_k)$ appears in the data $\\mathbf{X}$:\n",
    "\n",
    "$$ \\theta_{ijk} = \\cdots = \\frac{\\#\\mathbf{X}(X_i=x_{ij} \\wedge Y=y_k)}{\\#\\mathbf{X}(Y=y_k)} $$\n",
    "\n",
    "This estimate is the proportion of training samples in which $(X_i = x_{ij} \\wedge Y=y_k)$ appeared in $\\mathbf{X}$, looking only at samples consistent with $Y=y_k$.  The ellipsis $= \\cdots =$ in the formula above is used in place of the derivation that we saw in a previous lecture. \n",
    "\n",
    "\n",
    "#### Using the Bayesian Approach\n",
    "The Bayesian approach assumes a Beta prior for each $\\theta_{ijk}$ value, with hyper-parameters $a_{ik}, b_{ik}$ which can be specific to $X_i$.  There's no need to assume every $X_i$ must have the same hyper-parameters!\n",
    "\n",
    "$$ \\theta_{ijk} = \\cdots = \\frac{\\#\\mathbf{X}(X_i=x_{ij} \\wedge Y=y_k) + a_{ik}}{\\#\\mathbf{X}(Y=y_k) + a_{ik} + b_{ik}} $$\n",
    "\n",
    "The ellipsis $= \\cdots =$ in the formula above is used in place of the derivation that we saw in a previous lecture.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(Y)$ from Discrete data when $Y$ is a Discrete Random Variable\n",
    "\n",
    "The fact that $Y$ might have more than 2 values changes almost nothing.  There are more $\\theta_k$ to estimate, but the estimation is almost the same.\n",
    "\n",
    "#### Using Maximum Likelihood Estimates \n",
    "We  estimate $P(Y=y_k)$ by counting how often $Y=y_k$ appears in the data $\\mathbf{X}$:\n",
    "\n",
    "$$ \\theta_k = \\cdots =  \\frac{\\#\\mathbf{X}(Y=y_k)}{N} $$\n",
    "\n",
    "where $\\#\\mathbf{X}(Y=y_k)$ means the count of $Y=y_k$ in the data $\\mathbf{X}$.  \n",
    "\n",
    "\n",
    "#### Using the Bayesian Approach\n",
    "When $Y$ is binary, we could use a Beta prior over $\\theta_k$.  The simple extension of the Beta distribution to multiple outcomes is called a Dirichlet distribution  The Dirichlet distribution has hyper-parameters, one for each outcome.  The only complication is notation: In the Beta, we used $a,b$; for the Dirichlet, we have $a_0,\\ldots,a_k$.  We'll abbreviate the sum of these $A_k$ using $L$.\n",
    "\n",
    "$$ \\theta_k   = \\cdots = \\frac{\\#\\mathbf{X}(Y=y_k) + a_k}{N + L } $$\n",
    "\n",
    "where: \n",
    "* $\\#\\mathbf{X}(Y=y_k)$ means the count of $Y=y_k$ in the data $\\mathbf{X}$.\n",
    "* The values $a_{k}$ come fromthe Dirichlet prior for $\\theta_k$. \n",
    "* $L$ is the sum of the $a_k$.\n",
    "\n",
    "This result reduces to the Bianry model when $Y$ has 2 only values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(X_i|Y)$ from Discrete data when $Y$ is a Discrete Random Variable\n",
    "\n",
    "The fact that $Y$ might have more than 2 values changes almost nothing.  There are more $\\theta_k$ to estimate, but the estimation is almost the same.\n",
    "\n",
    "#### Using Maximum Likelihood Estimates \n",
    "As above, we can estimate $P(X_i = x_{ij}| Y=y_k)$ by counting how often the event $(X_i = x_{ij} \\wedge Y=y_k)$ appears in the data $\\mathbf{X}$:\n",
    "\n",
    "$$ \\theta_{ijk} = \\cdots = \\frac{\\#\\mathbf{X}(X_i=x_{ij} \\wedge Y=y_k)}{\\#\\mathbf{X}(Y=y_k)} $$\n",
    "\n",
    "This estimate is exactly the same as before.  We just have more values $x_{ij}$ to work through for each $y_k$.\n",
    "\n",
    "\n",
    "#### Using the Bayesian Approach\n",
    "As with $Y$ above, the only complication here is that we use a Dirichlet prior over $\\theta$ instead of a Beta, because $_i$ has more outcomes.  Because each $X_i$ might have 2 or discrete values, we need pseudo-counts for each combination of outcome for $X_i$ and $y_k$.  The only real problem is that writing the indices in the mathematics is a bit more complex.  We'll call each hyper-parameter or pseudo-count $a_ijk$.\n",
    "\n",
    "$$ \\theta_{ijk}  = \\cdots = \\frac{\\#\\mathbf{X}(X_i=x_{ij} \\wedge Y=y_k) + a_{ijk}}{\\#\\mathbf{X}(Y=y_k) + L_{ik}} $$\n",
    "and again, $L_{ik}$ is the sum of the $a_{ijk}$ for a specific $X_i$ and a specific outcome for $Y=y_k$.  \n",
    "\n",
    "The discussion about these pseudo-counts from the Binary case applies here as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(X_i|Y)$ from Discrete data when $Y$ is a Continuous Random Variable\n",
    "Remember that $Y$ has to be discrete for a classification task, but the features $X_i$ don't.  The simplest thing to do is assume that if $X_i$ is a continuous random variable, it obeys a Gaussian distribution conditional on the value of $Y=y_k$, with a mean $\\mu_{ik}$ and variance $\\sigma^2_{ik}$. \n",
    "\n",
    "#### Using Maximum Likelihood Estimates \n",
    "\n",
    "To estimate $\\mu_{ik}$, we can look at all the samples that have $Y=y_k$, and calculate the average value for $x_i$ over all those rows.  In computer science, there are many ways to extract the rows we need, but in math, the tradition is to invent an indicator function $\\delta()$ that's equal to 1 on the row we want, and equal to zero on the rows we don't want. \n",
    "$$ \\mu_{ik} = \\cdots = \\frac{\\sum_{j=0}^{N} x_{ij}\\delta(Y_j=y_k)}{\\sum_{j=0}^{N}\\delta(Y_j=y_k)}  $$\n",
    "where $j$ refers to the row number in $\\mathbf{X}$.  The numerator adds up the $x_i$ value for all the rows consistent with $Y=y_k$, and the denominator counts the number of rows consistent with $Y=y_k$.  \n",
    "\n",
    "Likewise, to estimate $\\sigma^2_{ik}$, we can look at all those rows and compute the average square difference from the mean:\n",
    "$$ \\sigma^2_{ik} = \\cdots = \\frac{\\sum_{j=0}^{N}(x_{ij} - \\mu_{ik})^2\\delta(Y_j=y_k)}{\\sum_{j=0}^{N}\\delta(Y_j=y_k)}$$ \n",
    "where $j$ again refers to the row number in $\\mathbf{X}$.  The numerator adds up the squared differences $(x_{ij} - \\mu_{ik})^2$ for all the rows consistent with $Y=y_k$, and the denominator counts the number of rows consistent with $Y=y_k$.\n",
    "\n",
    "\n",
    "These are maximum likelihood estimates.  The formulae above don't derive the formulae; they simply state the result of maximizing the log-likelihood, setting the first derivative to zero, and solving.  # Using the Bayesian Approach\n",
    "\n",
    "#### Using the Bayesian Approach\n",
    "We'll come back to this topic once we have explored regression!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting $P(X_i|Y)$ for Continuous $X_i$ in General \n",
    "We've explored the simplest models for estimating $P(X_i|Y)$.  If, by visualizing or understanding your data, you realize that your data set might be best modelled using something other than a Gaussian distribution, all you really need to find a way to estimate the density $X_i$ once for each value of $Y$.  There are tons of potential distributions, e.g., exponential, with well-known ways to estimate their parameters using maximum likelihood or the Bayesian approach.  \n",
    "\n",
    "When you take this approach, you almost certainly have to code it up yourself.  The method for fitting each $P(X_i|Y)$ might be different, but once you have a model for  $P(X_i|Y)$, you can apply the classifier as stated above without any change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
